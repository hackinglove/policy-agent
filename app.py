import streamlit as st
import pandas as pd
import sqlite3
import yaml
import time
import subprocess
import os
import json
from policy_agent.utils import load_config, load_sources
from policy_agent.source_detector import SourceDetector
from policy_agent.rag_engine import RAGEngine
from policy_agent.crawler import PolicyCrawler
from policy_agent.storage import Storage

# Page Setup
st.set_page_config(page_title="Policy Agent Dashboard", layout="wide")
st.title("ğŸ›ï¸ æ•°å­—ç»æµæ”¿ç­–é‡‡é›† Agent Dashboard")

# Load Config
config = load_config()

# Helper Functions
def save_config(new_config):
    with open('config.yaml', 'w', encoding='utf-8') as f:
        yaml.dump(new_config, f, allow_unicode=True)

def save_sources(new_sources):
    with open('sources.json', 'w', encoding='utf-8') as f:
        json.dump(new_sources, f, ensure_ascii=False, indent=2)

def run_crawler_subprocess():
    """Run crawler in a separate process"""
    cmd = [os.sys.executable, "main.py", "--now"]
    return subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

# Tabs
tab1, tab2, tab3, tab4 = st.tabs(["âš™ï¸ è®¾ç½®ä¸è¿è¡Œ", "ğŸ” æ”¿ç­–æŸ¥è¯¢", "â• æ·»åŠ æ¥æº", "ğŸ¤– AI åŠ©æ‰‹"])

# --- Tab 1: Settings & Control ---
with tab1:
    st.header("è¿è¡Œæ§åˆ¶")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("å®šæ—¶ä»»åŠ¡è®¾ç½®")
        current_time = config.get('schedule', {}).get('time', '09:00')
        new_time = st.text_input("æ¯æ—¥è¿è¡Œæ—¶é—´ (HH:MM)", value=current_time)
        
        if st.button("ä¿å­˜è®¾ç½®"):
            if 'schedule' not in config: config['schedule'] = {}
            config['schedule']['time'] = new_time
            save_config(config)
            st.success(f"å·²æ›´æ–°è¿è¡Œæ—¶é—´ä¸º: {new_time}")

    with col2:
        st.subheader("æ‰‹åŠ¨æ‰§è¡Œ")
        st.write("ç‚¹å‡»æŒ‰é’®ç«‹å³æ‰§è¡Œä¸€æ¬¡å…¨é‡æŠ“å–ä»»åŠ¡ã€‚ä»»åŠ¡å°†åœ¨åå°è¿è¡Œã€‚")
        if st.button("ğŸš€ ç«‹å³è¿è¡Œ"):
            with st.spinner("æ­£åœ¨å¯åŠ¨ä»»åŠ¡..."):
                process = run_crawler_subprocess()
                st.success(f"ä»»åŠ¡å·²å¯åŠ¨ (PID: {process.pid})")
                st.info("è¯·æŸ¥çœ‹ç»ˆç«¯æ—¥å¿—è·å–è¯¦ç»†è¿›åº¦ã€‚")

# --- Tab 2: Policy Query ---
with tab2:
    st.header("æœ¬åœ°æ”¿ç­–åº“æŸ¥è¯¢")
    
    # Connect DB
    db_path = "policy_data.db"
    if not os.path.exists(db_path):
        st.warning("æ•°æ®åº“å°šæœªåˆ›å»ºã€‚è¯·å…ˆè¿è¡Œä¸€æ¬¡æŠ“å–ä»»åŠ¡ã€‚")
    else:
        conn = sqlite3.connect(db_path)
        
        # Filters
        c1, c2, c3 = st.columns([2, 1, 1])
        search_text = c1.text_input("å…³é”®è¯æœç´¢ (æ ‡é¢˜/æ‘˜è¦)")
        
        # Get Source Names
        sources_df = pd.read_sql("SELECT DISTINCT source_name FROM policies", conn)
        source_options = ["æ‰€æœ‰éƒ¨é—¨"] + sources_df['source_name'].tolist()
        selected_source = c2.selectbox("å‘å¸ƒéƒ¨é—¨", source_options)
        
        # Query Construction
        query = "SELECT id, title, source_name, publish_date, url, summary FROM policies WHERE 1=1"
        params = []
        
        if search_text:
            query += " AND (title LIKE ? OR summary LIKE ?)"
            params.extend([f"%{search_text}%", f"%{search_text}%"])
            
        if selected_source != "æ‰€æœ‰éƒ¨é—¨":
            query += " AND source_name = ?"
            params.append(selected_source)
            
        query += " ORDER BY publish_date DESC LIMIT 100"
        
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()
        
        st.write(f"æ‰¾åˆ° {len(df)} æ¡è®°å½•")
        st.dataframe(
            df, 
            column_config={
                "url": st.column_config.LinkColumn("é“¾æ¥"),
                "summary": st.column_config.TextColumn("æ‘˜è¦", width="large"),
            },
            hide_index=True,
            use_container_width=True
        )

# --- Tab 3: Add Source ---
with tab3:
    st.header("æ·»åŠ æ–°æ”¿ç­–æº")
    
    st.info("è¾“å…¥ç›®æ ‡ç½‘å€ï¼ŒAI å°†å°è¯•è‡ªåŠ¨è¯†åˆ«æŠ“å–è§„åˆ™å¹¶æ·»åŠ è‡³ç³»ç»Ÿã€‚")
    
    new_url = st.text_input("æ”¿ç­–åˆ—è¡¨é¡µ URL", placeholder="https://example.gov.cn/policy/list.html")
    new_name = st.text_input("éƒ¨é—¨åç§°", placeholder="ä¾‹å¦‚ï¼šxxå¸‚å‘æ”¹å§”")
    
    if st.button("ğŸ¤– æ™ºèƒ½åˆ†æå¹¶æ·»åŠ "):
        if not new_url or not new_name:
            st.error("è¯·å¡«å†™å®Œæ•´ä¿¡æ¯")
        else:
            status_container = st.empty()
            status_container.info("æ­£åœ¨åˆ†æé¡µé¢ç»“æ„ï¼Œè¯·ç¨å€™...")
            
            # 1. Analyze
            detector = SourceDetector(config)
            selectors, err = detector.analyze(new_url)
            
            if err:
                status_container.error(f"åˆ†æå¤±è´¥: {err}")
            else:
                status_container.success("é¡µé¢åˆ†ææˆåŠŸï¼")
                st.json(selectors)
                
                # 2. Add to sources.json
                new_entry = {
                    "name": new_name,
                    "url": new_url,
                    "is_dynamic": True, # Assume dynamic for robustness or let user choose
                    "selectors": selectors
                }
                
                current_sources = load_sources()
                # Check duplicate
                if any(s['url'] == new_url for s in current_sources):
                    st.warning("è¯¥ URL å·²å­˜åœ¨äºæºåˆ—è¡¨ä¸­ã€‚")
                else:
                    current_sources.append(new_entry)
                    save_sources(current_sources)
                    st.success(f"å·²æ·»åŠ  '{new_name}' åˆ°é…ç½®æ–‡ä»¶ã€‚")
                    
                    # 3. Run Crawl for this source
                    st.write("æ­£åœ¨å°è¯•æŠ“å–è¯¥æºçš„å†å²æ•°æ®...")
                    try:
                        # Use a temporary config/source list to run just this one?
                        # Or instantiate Crawler with filtered sources list
                        storage = Storage()
                        crawler = PolicyCrawler(config, [new_entry], storage) # Make sure crawler supports partial list
                        new_items = crawler.run() # This runs in main process, might block UI
                        st.success(f"æŠ“å–å®Œæˆï¼å…±å‘ç° {len(new_items)} æ¡æ”¿ç­–ã€‚å·²å­˜å…¥æ•°æ®åº“ã€‚")
                    except Exception as e:
                        st.error(f"æŠ“å–æµ‹è¯•å¤±è´¥: {e}")

# --- Tab 4: AI Agent ---
with tab4:
    st.header("ğŸ¤– æ”¿ç­– AI åŠ©æ‰‹")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # React to user input
    if prompt := st.chat_input("è¯¢é—®æ”¿ç­–ç›¸å…³é—®é¢˜..."):
        # Display user message
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Get AI response
        with st.chat_message("assistant"):
            with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                rag = RAGEngine(config)
                response = rag.chat(prompt)
                st.markdown(response)
        
        st.session_state.messages.append({"role": "assistant", "content": response})
