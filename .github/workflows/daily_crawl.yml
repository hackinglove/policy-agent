name: Daily Policy Crawl

on:
  schedule:
    # 每天北京时间 09:00 (UTC 01:00) 自动运行
    - cron: '0 1 * * *'
  workflow_dispatch: # 允许手动触发

jobs:
  crawl_and_deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # 允许写入仓库 (发布 Pages, 更新 JSON)

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium

      - name: Restore DB from JSON
        run: |
          # 从 Git 中的 JSON 恢复数据库，保证历史数据不丢失
          python import_data.py

      - name: Run Crawler
        env:
          # 配置 secrets (需要在 GitHub 仓库设置中添加)
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} 
          # 如果使用了 PushPlus 或 Webhook
          PUSHPLUS_TOKEN: ${{ secrets.PUSHPLUS_TOKEN }}
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
        run: |
          # 运行主程序，--now 立即执行
          python main.py --now

      - name: Export Data for Frontend
        run: |
          # 将 SQLite 数据导出为 JSON 给静态页面使用
          python export_data.py

      - name: Commit and Push Data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Auto update policy data"
          file_pattern: "docs/*.json"

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs
          keep_files: true # 保持历史文件? 其实这里我们要覆盖
