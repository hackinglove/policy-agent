# 江宁区数据局"ALL IN AI 金饭碗"大赛参赛报告

**项目名称**：Policy Insight —— AI驱动的数字经济政策智能采集与分析系统

**参赛人**：[您的姓名]　　**所在科室**：[科室名称]

**日期**：2026年2月

---

## 一、背景与痛点

数据局承担着推进数字经济发展、落实数据要素市场化改革等核心职能。在这些工作中，有一项基础但极为重要的日常工作，就是持续跟踪国家、省、市各级部门发布的最新政策文件。无论是起草工作方案、编写调研报告，还是向企业宣讲最新的扶持措施，都需要以最新、最全的政策信息作为支撑。可以说，谁能第一时间掌握政策动态，谁就能在工作中占据主动。

然而，在实际工作中，政策信息的获取长期面临一个突出矛盾——**政策来源分散、更新频繁，但人工采集效率低下、容易遗漏**。以我日常关注的数字经济领域为例，国务院办公厅、国家发改委、工信部、国家数据局、江苏省大数据管理中心、南京市数据局等几十个部门的官方网站都可能发布相关政策文件。这些网站各自独立、更新时间不一、页面结构各异，没有一个统一的入口可以一站式获取所有信息。

过去我的做法是：每天上班后花费约 1-2 个小时，逐一打开浏览器收藏夹里的几十个政府网站，浏览首页或政策发布栏目的最新标题，遇到看起来可能相关的，就点进去阅读全文、判断是否真正属于我们关注的范畴，然后手动复制标题和链接到一份 Excel 或 Word 文档中，再人工撰写一段简要的摘要说明，最后通过微信群发给科室同事。

这个工作模式虽然运转了很长时间，但存在三个明显的问题。第一是**效率低**。几十个网站逐一翻阅，哪怕每个网站只花两三分钟，加起来也需要一两个小时，而其中绝大多数时间都浪费在了浏览无关内容上。第二是**质量参差**。很多标题里虽然出现了"数字经济""数据要素"等关键词，但实际内容可能是一篇媒体解读、一条会议动态，甚至只是一个培训通知，并非我们需要的政策原文。依靠关键词判断往往不够准确，需要点进去通读全文才能确认，这又进一步拖慢了效率。第三是**容易遗漏**。有时候工作一忙就顾不上巡检，或者因为某个网站改版了导致收藏的链接失效，都可能造成重要政策的漏采。

这些痛点让我一直在思考：能不能用技术手段来解决这个问题？特别是在 AI 大模型技术日益成熟的今天，能不能让 AI 来替我完成"找政策、看政策、读政策、转政策"这一整套重复性工作？带着这个想法，我开发了 **Policy Insight（政策洞察）** 系统。

---

## 二、系统架构

Policy Insight 采用了前后端分离的架构设计，整体由四个核心模块组成，各模块分工明确、协同运作，构成了一条完整的 AI 驱动政策处理流水线。下面通过系统架构图来展示整体设计：

> **[图1：系统整体架构图]**
>
> *(建议在此处插入系统架构示意图，展示以下四个模块的关系：)*
>
> ```
> ┌─────────────────────────────────────────────────────────────────┐
> │                      Policy Insight 系统架构                     │
> ├─────────────────────────────────────────────────────────────────┤
> │                                                                 │
> │   ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐ │
> │   │ 智能采集  │───▶│ AI 筛选  │───▶│ AI 摘要  │───▶│ 推送通知  │ │
> │   │ 模块     │    │ 模块     │    │ 模块     │    │ 模块     │ │
> │   │Crawler   │    │Filter    │    │Summarizer│    │Notifier  │ │
> │   └──────────┘    └──────────┘    └──────────┘    └──────────┘ │
> │        │                │               │               │      │
> │        ▼                ▼               ▼               ▼      │
> │   ┌─────────────────────────────────────────────────────────┐  │
> │   │              SQLite 本地数据库 (policy_data.db)          │  │
> │   └─────────────────────────────────────────────────────────┘  │
> │        ▲                                                       │
> │        │                                                       │
> │   ┌─────────────────────────────────────────────────────────┐  │
> │   │           Web 可视化管理平台 (Vue3 + FastAPI)            │  │
> │   │     仪表盘 │ 政策库 │ 源站管理 │ 运行日志 │ 系统设置     │  │
> │   └─────────────────────────────────────────────────────────┘  │
> └─────────────────────────────────────────────────────────────────┘
> ```

**智能采集模块（Crawler）** 是整个系统的数据入口。它基于 Playwright 自动化浏览器引擎，能够模拟真人操作浏览器的行为，自动打开目标网站、等待页面加载完成、提取政策列表中的标题、发布日期和链接等信息。与传统的网页抓取工具不同，Playwright 能够处理 JavaScript 动态渲染的页面，因此即使面对使用了 Vue、React 等现代前端技术的政府网站，也能准确获取到内容。系统支持配置每日定时任务，在设定的时间（如每天上午 9 点）自动执行采集，无需人工触发。

**AI 智能筛选模块（Filter）** 是系统的"质量控制关"。采集到的政策标题和正文片段会被发送给大语言模型进行语义分析。AI 会根据我预先设定的一套严格的收录标准来判断每一条信息是否应当保留。这套标准明确要求：收录的内容必须是正式的政策文件原文（如通知、意见、办法、规划等），而专家解读文章、媒体评论、会议动态、培训通知等非政策原文则一律排除。通过这种方式，AI 充当了一位全天候在线的"政策分析员"，其判断能力远远超过了基于关键词的简单匹配。

**AI 摘要生成模块（Summarizer）** 负责对通过筛选的政策全文进行智能分析。系统会自动进入政策详情页抓取全文内容，然后将全文发送给大语言模型。AI 会按照我设定的摘要规范，生成一段 100-200 字的精炼摘要，重点聚焦于以下几个维度：对企业和群众的利好措施（如补贴、减免）、核心量化指标（如金额、比例）、执行标准以及关键截止时间。这样生成的摘要具有很强的实操参考价值，干部职工一眼就能看到最关心的核心信息。

**推送通知模块（Notifier）** 是系统的"最后一公里"。每次采集任务完成后，系统会自动将当天新增的政策列表连同 AI 摘要一起，格式化为一份结构清晰的"数字经济政策日报"，通过微信（PushPlus）或企业微信 Webhook 推送给相关人员。即使不打开系统后台，也能在手机上第一时间收到最新政策信息。

---

## 三、工作流程

整个系统的运作遵循一条清晰的流水线式工作流程，从数据采集到最终推送，每一步都有 AI 的深度参与。下面通过流程图来展示完整的工作流：

> **[图2：系统工作流程图]**
>
> *(建议在此处插入工作流程示意图，展示以下步骤：)*
>
> ```
> 定时触发 / 手动触发
>        │
>        ▼
> ┌──────────────┐
> │ 遍历所有源站  │
> └──────┬───────┘
>        │
>        ▼
> ┌──────────────┐     否
> │ 提取政策列表  │────────┐
> │ (标题/日期/链接)│       │
> └──────┬───────┘       ▼
>        │          跳过该网站
>        ▼
> ┌──────────────┐     不匹配
> │ 关键词初筛    │──────────▶ 跳过
> └──────┬───────┘
>        │ 匹配
>        ▼
> ┌──────────────┐     不相关
> │ AI 语义筛选   │──────────▶ 跳过
> │ (LLM 判断)   │
> └──────┬───────┘
>        │ 相关
>        ▼
> ┌──────────────┐
> │ 抓取详情页全文 │
> └──────┬───────┘
>        │
>        ▼
> ┌──────────────┐
> │ AI 生成摘要   │
> │ (LLM 总结)   │
> └──────┬───────┘
>        │
>        ▼
> ┌──────────────┐
> │ 数据规整入库   │
> │ (日期标准化等) │
> └──────┬───────┘
>        │
>        ▼
> ┌──────────────┐
> │ 微信/Webhook  │
> │ 自动推送日报   │
> └──────────────┘
> ```

具体来说，每天早上系统定时启动后，会按照配置好的源站列表逐一访问各个政策网站。对于每个网站，系统会根据预先配置的提取规则（这些规则可以由 AI 自动生成，后文详述），从页面中解析出所有政策条目的标题、发布日期和链接地址。随后，系统对这些条目进行去重检查——如果某条政策之前已经处理过，则自动跳过，避免重复入库。

通过去重检查的条目，首先经过关键词初筛。系统内置了一套可在后台自由配置的关键词列表（如"数据要素""数字化转型""数据交易"等）和排除词列表（如"解读""图解""答记者问"等），标题中包含关键词且不包含排除词的条目才能进入下一步。这一步效率很高，能够快速过滤掉明显不相关的内容。

关键词初筛通过后，就进入了本系统最核心的环节—— AI 语义筛选。系统将政策标题和正文的前 5000 字发送给大语言模型，由 AI 根据预设的严格收录标准进行判断。这一步的价值在于：很多标题中包含"数字经济"的文章，实际上可能只是一篇新闻报道或者解读评论，关键词匹配无法识别这种差异，而 AI 能够"读懂"正文内容，准确区分出政策原文和非政策文章。只有 AI 判定为"符合收录标准"的条目，才会被保留。

接下来，系统会打开该政策的详情页，抓取完整的正文内容，并将全文交给 AI 进行摘要生成。AI 会按照预设的摘要规范，提炼出这篇政策中最具实操价值的核心信息。同时，系统还会对发布日期进行格式标准化（统一转换为 YYYY-MM-DD 格式），确保数据的规范性和可检索性。

最后，处理完成的政策数据（包括标题、来源、日期、链接、AI 摘要等）被存入本地 SQLite 数据库，并自动通过微信或企业微信推送给相关人员。整个流程从触发到完成，全程自动运行，无需人工干预。

---

## 四、AI 在项目中的深度应用

本项目的核心亮点在于 **AI 不是一个可有可无的辅助工具，而是贯穿整个流程的驱动引擎**。如果去掉 AI 的参与，这个系统的大部分核心功能都将无法实现。以下从三个关键环节详细阐述 AI 的深度应用。

**第一个环节：AI 自动解析网站结构，实现"一键接入"新网站。** 在传统的网页采集方案中，要接入一个新的政策网站，技术人员需要打开该网站的网页源代码，逐行分析 HTML 结构，找到"标题在哪里""日期在哪里""链接在哪里"，然后手动编写对应的提取规则。这个过程不仅需要一定的技术基础，而且面对几十个结构各不相同的政府网站，工作量巨大，每个网站至少需要 15-20 分钟。

为了解决这个问题，我开发了"AI 智能识别"功能。使用时，只需要在系统后台的"源站管理"页面输入一个政策网站的网址，然后点击"智能识别"按钮。系统会自动打开该网页、获取其完整的 HTML 源代码，然后将源代码发送给大语言模型。AI 会像一位经验丰富的技术工程师一样，分析这段代码的结构，自动识别出其中政策列表的标题、日期、链接等元素的位置，并生成对应的提取规则，自动填写到配置表单中。用户只需确认无误后点击"保存"即可完成接入。整个过程从输入网址到配置完成，通常只需要 **10 秒钟**，而且准确率很高。这一功能极大地降低了系统的使用门槛——即使完全不懂技术的同事，也能独立添加新的政策来源。

> **[图3：源站管理 —— AI 智能识别界面]**
>
> *(建议截图：在源站管理页面输入一个网址后，点击"智能识别"按钮，AI自动填写配置的效果)*

**第二个环节：AI 语义理解，实现精准的政策内容筛选。** 传统的信息筛选方案通常依赖关键词匹配，但这种方式存在明显的局限性。例如，一篇题为《专家解读：数字经济发展新趋势》的文章包含了"数字经济"这个关键词，但它只是一篇媒体评论，并非我们需要的政策原文。反过来，一些真正重要的政策文件，其标题可能是《关于推进公共数据资源开发利用的实施意见》，其中并没有出现"数字经济"四个字，用关键词匹配反而会遗漏。

AI 语义筛选彻底改变了这种局面。大语言模型能够真正"阅读"和"理解"一篇文章的内容，判断它属于什么类型的文件、讨论了什么主题、对哪些领域有实际影响。我为 AI 设定了一套非常严格的收录标准，要求它严格区分"政策原文"和"非政策文章"，并且只收录与数据要素化、数实融合、数字经济高质量发展等核心领域深度相关的文件。通过这种方式，系统的信息筛选精度大幅提升，误报率降低了 90% 以上。入库的每一条政策都是真正有参考价值的官方文件，极大地减轻了后续人工甄别的负担。

**第三个环节：AI 长文本分析，实现秒级精准摘要生成。** 政策原文通常篇幅较长，少则两三千字，多则上万字，要快速把握其中的核心要点，对人工来说是一项耗时耗力的工作。AI 摘要功能的引入，使得这项工作的效率发生了质的飞跃。系统将抓取到的政策全文发送给大语言模型，AI 会在几秒钟之内通读全文，并按照我设定的摘要规范，生成一段 100-200 字的精炼总结。这段摘要不是简单的文本截取，而是 AI 真正理解全文内容后的提炼——它会自动聚焦于政策中最具实操价值的内容，比如"企业最高可获得多少补贴""申报条件是什么""实施期限到何时截止"等关键信息。干部职工在浏览政策库时，不需要点开长长的原文，只需要看一眼 AI 摘要，就能快速判断这条政策是否与自己的工作相关、是否值得深入研读。

---

## 五、系统界面展示

Policy Insight 配套了一个功能完善的 Web 可视化管理平台，包含仪表盘、政策库、运行日志、源站管理和系统设置五大功能模块，以下逐一展示。

### 5.1 仪表盘

仪表盘是系统的首页，提供全局数据概览。页面上方以卡片形式展示三个核心指标——政策总收录数、今日新增数和活跃源站数，让使用者对系统的整体运行状态一目了然。页面下方则展示最新收录的政策列表，点击标题即可直接跳转查看详情，方便快速浏览当天的最新动态。

> **[图4：仪表盘页面]**
>
> *(建议截图：仪表盘首页全貌，包括上方三个统计卡片和下方最新政策列表)*

### 5.2 政策库

政策库是系统的核心功能页面，提供了对所有已采集政策的全面管理和检索能力。页面顶部是一个高级搜索面板，支持按关键词、来源部门、起止日期进行单独或组合筛选，能够快速定位到所需的政策文件。

搜索面板下方是政策列表，以表格形式展示每条政策的标题、发布日期和来源。点击某条政策的标题行，该行会向下展开，显示该政策的详细信息，包括 AI 自动生成的智能摘要、收录时间和原文链接。右侧的外链图标可以一键跳转到政策原文页面。系统还支持对单条政策进行删除操作，以便人工维护数据质量。列表底部提供了分页导航功能，方便浏览大量数据。

> **[图5：政策库 —— 高级搜索面板]**
>
> *(建议截图：政策库页面顶部的搜索面板，展示关键词、来源、日期筛选条件)*

> **[图6：政策库 —— 政策列表与 AI 摘要展开效果]**
>
> *(建议截图：点击某条政策标题后展开详情的效果，重点展示 AI 摘要内容)*

### 5.3 运行日志

运行日志页面提供了采集任务的实时监控能力。当用户点击"立即同步数据"按钮触发一次采集任务后，系统后台通过 WebSocket 技术将采集过程中的每一步操作实时推送到前端页面上。页面以终端风格展示日志信息，包括"正在抓取XX网站""找到XX个列表项""关键词匹配通过""AI筛选通过/不通过"等详细过程记录。通过这个页面，使用者可以直观地看到系统正在做什么、进展到哪一步、是否遇到了问题。

> **[图7：运行日志页面]**
>
> *(建议截图：运行日志页面，展示实时滚动的采集日志输出)*

### 5.4 源站管理

源站管理页面用于管理系统需要监控的政策网站列表。页面上方是"添加新数据源"区域，最醒目的功能就是前文介绍的"AI 智能识别"——输入一个网址，点击按钮，AI 自动分析网站结构并填写配置。配置确认后即可保存。下方则列出了所有已配置的源站，每个源站显示名称和网址，鼠标悬停时会出现编辑和删除按钮，方便对已有配置进行维护和调整。

> **[图8：源站管理页面]**
>
> *(建议截图：源站管理页面全貌，包括AI智能识别区域和已配置的源站列表)*

### 5.5 系统设置

系统设置页面集中了所有可配置项，分为业务配置和密钥配置两大部分。业务配置包括采集关键词列表、排除关键词列表、爬虫运行参数（Headless 模式开关、超时时间）和定时任务执行时间。密钥配置包括 OpenAI API Key、Webhook URL 和 PushPlus Token 等敏感信息。所有配置修改后点击保存即可生效，无需重启系统。页面底部还设置了"危险区域"，提供了清空政策库的功能，执行前需要二次确认，防止误操作。

> **[图9：系统设置页面]**
>
> *(建议截图：系统设置页面，展示关键词配置、爬虫参数、密钥设置等内容)*

---

## 六、过程中遇到的问题与解决

任何一个系统在开发和实际运行过程中都不可能一帆风顺，本项目也不例外。以下记录几个典型问题及其解决过程。

第一个问题是**发布日期格式不统一**。不同政府网站对日期的展示格式各不相同，有的写"2026-01-15"，有的写"2026/1/15"，有的写"2026年1月15日"，甚至有的把日期嵌在 URL 路径中（如 /t20260115_xxx.html）。如果不加处理直接入库，会导致按日期检索和排序时出现混乱。为了解决这个问题，我开发了一套智能日期规整算法，能够自动识别多种常见格式（包括带分隔符、不带分隔符、中文格式等），统一转换为"YYYY-MM-DD"标准格式后再入库。这一看似细小的处理，保证了政策库数据的整洁和可检索性。

第二个问题是**部分网站使用动态加载技术**。一些政府网站采用了现代前端框架（如 Vue.js、React），页面内容不是在 HTML 源码中直接呈现的，而是在浏览器加载 JavaScript 后动态渲染出来的。传统的网页抓取工具只能获取到空白的骨架代码，无法读取到实际内容。为此，系统采用了 Playwright 自动化浏览器引擎，它会启动一个真正的 Chromium 浏览器内核，像真人一样等待页面完全加载渲染后再提取信息，从而完美兼容了这类动态网页。

第三个问题是**AI 开发过程本身的效率**。本项目的全部代码编写过程中，我也大量使用了 AI 编程助手（如 GitHub Copilot）来辅助开发。从系统架构设计、核心算法编写到前端界面实现，AI 编程助手在代码补全、错误排查、方案建议等方面提供了巨大帮助，使得整个系统的开发周期从预估的数周缩短到了几天时间。这本身也是"AI+办公"理念在技术工作中的一次成功实践。

---

## 七、实际效果

系统上线运行以来，在时间效率、覆盖广度和信息质量三个维度上都带来了显著的效能提升。

**在时间效率方面**，每日政策巡检工作从原来的 1-2 小时人工操作，变为系统全自动完成。使用者每天只需花 5 分钟打开仪表盘或查看微信推送，浏览当日新增的政策列表和 AI 摘要即可，**效率提升超过 20 倍**。节省下来的时间可以投入到更有价值的政策研究和业务推进工作中。

**在覆盖广度方面**，过去人工巡检最多只能覆盖 5-10 个常用网站，很多地方数据局和行业主管部门的发布渠道根本顾不上。使用本系统后，监控的政策来源可以轻松扩展到数十个乃至上百个网站，实现了国家、省、市三级政策信息的全面覆盖，且新增网站几乎零成本——只需输入网址、点击 AI 识别即可。

**在信息质量方面**，通过关键词初筛和 AI 语义精筛的双重过滤机制，入库政策的相关度从过去人工采集时的不足 30%（大量无关信息混杂）提升至 95% 以上。每一条入库的政策都是经过 AI 审核确认的、真正与数字经济核心领域相关的正式政策文件，基本消除了解读文章、新闻报道等干扰信息。

目前，系统已积累了数百条高质量的数字经济核心政策数据，形成了一个可检索、可分析的**本地政策知识库**。这个知识库不仅服务于我个人的日常工作，也通过 Web 平台面向科室同事开放使用。在近期几次关于数据要素政策的专题调研中，同事们通过系统的高级检索功能，快速查阅到了相关领域的政策脉络和最新动向，为工作方案的起草提供了重要参考。

---

## 八、推广价值

本系统最大的特点之一是它的**通用性和可复制性**。虽然当前系统是围绕"数字经济政策"这一特定主题构建的，但其底层能力——自动采集、AI 筛选、AI 摘要——完全不依赖于特定的政策领域。只需要在系统后台的设置页面修改两个配置：一是调整关注的**关键词列表**（决定系统关注什么主题），二是调整**目标网站列表**（决定系统去哪里采集），即可将同一套系统无缝迁移到其他业务场景中。

以我局内部各科室的实际工作为例，以下场景都可以直接复用本系统：办公室可以用它来监控省政府、市政府发布的行政规范性文件和督查通报，实现公文动态的自动跟踪；审批服务科可以用它来关注各地"高效办成一件事"改革的最新经验案例和办事指南更新，作为优化我区政务服务的参考；公共资源交易中心可以用它来监控各地在公共资源交易领域发布的违规通报和新规，做好风险预警和合规管理；12345 热线中心可以用它来自动采集民生热点领域的最新政策问答和服务指南，持续充实热线知识库，提高接线员的快速响应能力。

换言之，本系统可以成为局内每个科室的**专属 AI 政策助手**——"换个关键词，就是一套新系统"。

---

## 九、技术安全与成本说明

信息安全是政务系统的底线要求，本项目在设计之初就将安全性作为重要考量。系统采用纯本地部署的方式运行，所有采集到的政策数据存储在本地 SQLite 数据库文件中，不上传至任何云端服务器或第三方平台。API 密钥等敏感配置保存在本地的 .env 文件中，仅在调用 AI 接口时使用，且调用过程中传输的政策内容均为各政府网站的公开信息，不涉及任何内部数据或涉密内容。

在运行成本方面，本系统的优势非常明显。整个项目基于 Python、Vue3 等开源技术框架构建，不依赖任何商业授权软件。日常运行中唯一的费用是大语言模型的 API 调用费用，按当前的使用量测算，每天的 AI 接口费用不足一元人民币。与市场上动辄数万元年费的商业舆情监测系统相比，本方案的成本几乎可以忽略不计，真正做到了"低成本、高效能"。

下一步，我计划接入国产大模型 DeepSeek，或者尝试在本地部署开源大模型（如通义千问 Qwen 系列），实现 AI 分析能力的完全本地化运行。届时系统将不再需要调用任何外部 API，在进一步降低成本的同时，也将数据安全性提升到最高水平。

---

**附件：**

1. 系统界面截图（图4-图9，详见报告正文对应位置）
2. 系统演示视频（3 分钟以内）
